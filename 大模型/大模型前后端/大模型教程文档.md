 介绍 



LangChain4j 的目标是简化与 Java 应用程序 集成大模型。



 特性： 

●统一 API： LLM提供程序（如 OpenAI 或 阿里百炼）和嵌入（向量）存储（如 redis 或 ES） 使用专有 API。LangChain4j 提供了一个统一的 API，以避免为每个 API 学习和实现特定的 API。 要试验不同的LLMs存储或嵌入的存储，您可以在它们之间轻松切换，而无需重新编写代码。 LangChain4j 目前支持 [15+ 热门LLM](https://docs.langchain4j.dev/integrations/language-models/) 和 [20+ 嵌入模型](https://docs.langchain4j.dev/integrations/embedding-stores/)。



langchain4j vs springAI

| 维度       | Spring AI                    | LangChain4j            |
| ---------- | ---------------------------- | ---------------------- |
| ‌技术栈绑定‌ | 强依赖 Spring 生态           | 无框架依赖，可独立使用 |
| 适用场景   | SpringBoot应用快速接入单模型 | 多模型（动态模型）平台 |

 初识LangChain4j(纯java) 

接下来，让我们与LangChain4j初识一下，新建一个Maven工程，然后添加以下依赖：



引入了langchain4j的核心依赖、langchain4j集成OpenAi各个模型的依赖。

 和OpenAi的第一次对话 



运行代码结果为：

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1740895623613-c1e9fcc4-3f18-4e90-85b1-9fc29ffe6d4a.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





你会发现，  LangChain4j 对于初次接入大模型的开发者来说十分友好，不需要指定模型，不需要指定apikey, 即可对接大模型进行对话，这是怎么做到的呢？

 其实我们对ApiKey为"demo" ， 底层会做这些事情：







在底层在构造OpenAiChatModel时，会判断传入的ApiKey是否等于"demo"，如果等于会将OpenAi的原始API地址"https://api.openai.com/v1"改为"http://langchain4j.dev/demo/openai/v1"，这个地址是langchain4j专门为我们准备的一个体验地址，实际上这个地址相当于是"https://api.openai.com/v1"的代理，我们请求代理时，代理会去调用真正的OpenAi接口，只不过代理会将自己的ApiKey传过去，从而拿到结果返回给我们。

所以，真正开发时，需要大家设置自己的apiKey或baseUrl，可以这么设置：



 接入deepseek 



 文生图WanxImageModel 



 文生语音 



 整合SpringBoot 

先引入SpringBoot：



 接入百炼 

官网：

![img](https://docs.langchain4j.dev/img/favicon.ico)

DashScope (Qwen) | LangChain4j







Controller：



 配置通义千问-Max模型： 



访问http://localhost:8080/ai/chat：

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1740990111656-493050cd-bbcb-4a13-9dbd-a715aaa36808.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_15%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



 配置deepseek模型 



访问http://localhost:8080/ai/chat：

您好!我是由中国的深度求索(DEPSEEK)公司开发的智能助手DEEPSEKR1,如您有任何任何问题,我会尽我所能为您是供帮助.

TTP://LOCALHOST:8080AI/CHAT

GHTTP://LOC

N

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1740990303915-368abb51-9450-4f3b-8ba2-ecbcc304de19.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





 接入Ollama 

关于Ollama的本地部署：  



DeepSeek本地部署教程



官网：

![img](https://docs.langchain4j.dev/img/favicon.ico)

Ollama | LangChain4j



 





Controller：



 配置通Deepseek模型： 



访问http://localhost:8080/ai/chat：

您好!我是由中国的深度求索(DEPSEE)公司开发的督能励手DEEPSEKR1,如您有任何任何问题,我会尽我析能为您提供帮助.

COHTTP://LOCALHOST:8O8O/AI/CHATOLLAMA

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1740993232428-2c10a8b7-4ece-4858-bfb2-08b9af3c5184.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_30%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)









 流式输出 

因为langchain4j不是spring家族， 所以我们在wen应用中需要引入webflux





通过Flux进行流式响应



到这里你会发现，  langchain4j毕竟不是spring家族， 和spring生态一起用真蹩脚。  还是springai舒服

 记忆对话(多轮对话） 

 原生方式 

大模型并不会把我们每次的对话存在服务端， 所以他记不住我们说的话



响应：

你好,徐庶老师!很高兴与您交流.请问有什么我可以帮助您的?

抱款,我无法知道你的名字.如果你愿意,可以告诉我你的名字.

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741075968275-a439130c-0c51-43c1-83d2-0f175ec0e327.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_17%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





所以每次对话都需要将之前的对话记录，都发给大模型， 这样才能知道我们之前说了什么：



你才说你叫徐底.如果这是你的名字,那么我会称呼你为徐底.如果有其他想法或问题,请随时告诉我!

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741076057161-2353ac04-9273-48d3-9b90-2adf7636bcc8.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





但是如果要我们每次把之前的记录自己去维护， 未免太麻烦， 所以提供了ChatMemory

但是他这个ChatMemory没有SpringAi好用、易用， 十分麻烦！  所以说谁在跟我说Langchain4j比SpringAi好我跟谁急！。

 通过ChatMemory 

在SpringBoot中他要这么用：



原理：

0. 通过AiService创建的代理对象（

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741534879948-8736e133-3117-434b-9374-2c65394893f5.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_11%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

）调用chat方法

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741534885030-44d3689a-973f-40bc-9c80-4e914354eaf8.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_9%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



1代理对象会去ChatMemory中获取之前的对话记录（获取记忆）

2将获取到的对话记录合并到当前对话中（此时大模型根据之前的聊天记录肯定就拥有了“记忆”）

3将当前的对话内容存入ChatMemory（保存记忆）

CHATLANGUAGEMODEL.CHAT(我是谁)一3当前对话内容存入记忆中

动态代理:AISERVICES.BUILDER(XUSHUCHATMODEL.CL

LANGCHAIN4J

基础大模型

应用程序

XUSHUCHATMODEL.CHAT

传入到当前对话中

CHATMEMOR

夏

Q获取记忆

MAP

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741533176237-b8bf90c6-c57d-469b-bb80-b2fa8a60d1f2.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_34%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





Controller:



这封装得也太不优雅了！   看看人家Spring-AI 封装得那叫一个优雅， 行吧不吐槽了，我们继续往下面看吧。



我们通过2种接口体验记忆对话， （当然也可以通过同一个接口）

访问：/memory_chat 

庶的经历和选择有深入讨论的兴趣,我很乐意与您交流!请问您具体想了解什么呢?

狭特而无奈转投曹营,留下了身在曹营心在汉"的典故,若您想探过三国历史,人物故事,或对徐

您好!徐麻是三国时期害名的谋士,以智谋和孝义闻名.史书记载您曾为浏备效力,后因母亲被曹操所

OHTTP://LOCALHOST:8080/AIOTHER/MEMORYC

命

C

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741087660400-6dfecac7-073c-4d32-9b39-0d16afbfd8c6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



访问：/memory_stream_chat   

您刚刚提到您叫徐庶.如果您有其他问题或想进一步介绍自己,欢迎继续分

享!我可以根据您的需求提供帮助或进行交流.

HTTP://LOCALHOST:8080/AIOTHER/MEMORYSTREAM

CA

OHTTF

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741087679522-15b1437c-d42b-4eb7-84cb-5175b8395cd0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





 

 记忆分离 

现在我们再来想另一种情况：  如果不同的用户或者不同的对话肯定不能用同一个记忆，要不然对话肯定会混淆，此时就需要进行区分：



可以通过memoryId进行区分， 





原理：

0. 通过AiService创建的代理对象（

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741534879948-8736e133-3117-434b-9374-2c65394893f5.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_11%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

）调用chat方法

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741535033465-b5fb0b12-9f55-494d-9777-4b88da638bd0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_11%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)

传入id



1代理对象会去ChatMemory中根据id获取之前的对话记录（获取记忆）

2将获取到的对话记录合并到当前对话中（此时大模型根据之前的聊天记录肯定就拥有了“记忆”）

3将当前的对话内容根据id存入ChatMemory（保存记忆）

CHATLANGUAGEMODELCHAT(我是谁3当前对话内容存入当前ID记忆中

动态代理:AISERVICES.BUILDER(XUSHUCHATMODELCLASS)

LANGCHAIN4J

传入到当前对话中

(1:["我是徐庶,我是谁]

基础大模型

XUSHUCHATMODEL.CHATID

MAP<IDLIST<STRING>>

应用程序

G根据ID获取记忆

1

CHATMEMORY

夏

MAP:

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741533313060-81ede4c2-db59-4e62-b17f-5e72a2a35845.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_34%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



memoryId可以设置为用户Id, 或者对话Id  进行区分即可：







看效果：

userId=1，我叫徐庶

徐庶是三国时期薯名的谋士和忠臣,历史上以智谋和孝义闻名.

HTTP:/LOCALHOST:8080/AIOTHER/MEMORYLDCHAT?MESSAGE-我徐&USERLD=1

N

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741088228580-b1157367-ab62-4feb-97f2-5d314db95a50.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_22%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



userId=2, 我叫什么

之前的聊天记录.如果您愿意告诉我您的名字,我会很乐意在接

很抱歉,我无法去知道您的名字,因为我无法访问您的个人信息或

HTTP://LOCALHOST:8080/AIOTHER/MEMORYLDCHAT?MESSAGE=我什么&USERLD=2

下来的对话中使用它来称呼您!

N

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741088287746-75e55684-7bca-4c76-a3aa-add22ffd46e1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



userId=1  ，我叫什么

史意涵,还是想以这个名字为起点探讨其他话题呢?

您刚才告诉我您叫**徐庶*,需要我为您解读这个名字背后的历

TTP://LOCALHOST:8080/AIOTHER/MEMORYLDCHAT?MESSAGE=我什么&USERLD=1

HTTP://LOCALHOST:8O8OAIOTHE

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741088364504-1f3ee2be-9ed5-40b3-83a0-bcc901ef45ff.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_23%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)







 持久化对话 

OK, 完成！   如果要对记忆的数据进行持久化呢？ 因为现在的数据其实是存在内存中， 重启就丢了



可以配置一个ChatMemoryStore 

默认是InMemoryChatMemoryStore——通过一个map进行存储，

LICCLASSINNENORYCHATMENORYSTOREINPLENENTSCHATNENORYSTONE

PRIVATEFINA1MAP<0BJECT,LIST<CHATMESSAGE>>

\>ESSAGESBYMEMORYID=NEWCONCURRENTHASHMAP<>();

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741088608226-3d4d8f59-020c-4a30-89b0-d18813dcd1c0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_32%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



 所以如果需要持久化到第三方存储， 可以重新配置ChatMemoryStore ：



1自定义ChatMemoryStore实现类：  假设持久化到数据库 ， 具体代码我不演示了



2然后配置ChatMemoryStore



 Function-call（Tools） 

对于基础大模型来说， 他只具备通用信息，他的参数都是拿公网进行训练，并且有一定的时间延迟，  无法得知一些具体业务数据和实时数据， 这些数据往往被各软件系统存储在自己数据库中：



比如我问大模型：“中国有多少个叫徐庶的” 他肯定不知道，  我们就需要去调用政务系统的接口。

比如我现在开发一个智能票务助手，  我现在跟AI说需要退票， AI怎么做到呢？  就需要让AI调用我们自己系统的退票业务方法，进行操作数据库。



那这些都可以通过function-call进行完成，更多的用于实现类似智能客服场景，因为客服需要帮用户解决业务问题（就需要调用业务方法）。

 



function-call的流程：

比如： 我现在需要当对话中用户问的是“长沙有多少个叫什么名字”的对话， 我需要去我程序中获取

1问大模型 长沙有多少个叫徐庶的

2大模型在识别到你的问题是： “长沙有多少个叫什么名字”

3大模型提取“徐庶”

4调用changshaNameCount方法

5通过返回的结果再结合上下文再次请求大模型

6响应“长沙有xx个叫徐庶的”

 .调用CHANGSHANAMECOUNT(NAME)基础大模型应用程序LANGCHAIN-4J.再让A组织语言-4.调用一长沙有多少个CHATGPTNAME:余麻返回结果徐的![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741158666304-19c54b46-1e75-43f6-93fa-3175e15f7f47.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp) 



 实现： 

1加入回调方法：



●ToolsService配置为了一个bean

●@Tool  用于告诉AI什么对话调用这个方法

●@P("姓名")  用于告诉AI ，调用方法的时候需要提取对话中的什么信息， 这里提取的是姓名

2结合通过AiService配置tools ， 这里用的是前面记忆对话时配置的Assistant





所以， 你如果需要加更多的tool.  只需要在TollsService中加， 比如：   

这个langchan4j封装得倒是挺易用的👍





 预设角色（系统消息SystemMessage）： 

基础大模型是没有目的性的， 你聊什么给什么， 但是如果我们开发的事一个智能票务助手， 我需要他以一个票务助手的角色跟我对话，  并且在我跟他说"退票"的时候，  让大模型一定要告诉我“车次”和"姓名"  ，这样我才能去调用业务方法（假设有一个业务方法，需要根据车子和姓名才能查询具体车票），进行退票。

CANCELBOOKING(BOOKINGNUMBERNAME)

OOKINGNUMBER:G131

BOOKINGNURBERG131NAME徐底

基础大模型

票务助手

3.次:G131姓名:徐庭

CHATGPT

FUNCTION-CAL

NAME:徐庶

1

2请提纸

数据库

车次和性名

1票

4.

![langchain4j.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741698167332-9f35ff5a-8472-4585-af8d-f09e20deb17b.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



在langchain4j中实现也非常简单

● @SystemMessage 系统消息， 一般做一些预设角色的提示词，设置大模型的基本职责

●可以通过{{current_date}} 传入参数，  因为预设词中的文本可能需要实时变化

●@V("current_date")， 通过@V传入{{}}中的参数

●一旦参数不止一个， 就需要通过@UserMessage设置用户信息



HTTP:/LOCALHOST:8080/AI/MEMORYSTREAMCHATMESSAGE=大日期是

天的日期是2025年3月11日.有什么我可以帮您的吗?

价

G

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741697382482-7c9eba58-9b6d-4d3b-9974-8ae61b425f49.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_26%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



COHTTP://LOCALHOST:8080/AI/MEMORYSTREAM

我们继续之前,我会先向您说明相关细节.请您提供所需的信息吧

OSLEAMCHATMESSAGE-

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741697402313-3f451310-3321-4b67-ae0c-b71478eadc23.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_50%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





另外：假设大模型不支持系统消息（一般都支持），可以用@UserMessage代替@SystemMessage 



 RAG： 

检索增强生成（Retrieval-augmented Generation）

对于基础大模型来说， 他只具备通用信息，他的参数都是拿公网进行训练，并且有一定的时间延迟，  无法得知一些具体业务数据和实时数据， 这些数据往往在各种文件中（比如txt、word、html、数据库...）



虽然function-call、SystemMessage可以用来解决一部分问题

但是它只能少量，   如果你要提供大量的业务领域信息，  就需要给他外接一个知识库：



比如

1我问他退订要多少费用

2

这些资料可能都由产品或者需求编写在了文档中：  



terms-of-service.txt(1 kB)



a所以需要现在需求信息存到向量数据库（这个过程叫Embedding， 涉及到文档读取、分词、向量化存入）

3去向量数据库中查询“退订费用相关信息”

4将查询到的数据和对话信息再请求大模型

5此时会响应退订需要多少费用

美元,豪华经济舱50美元,商务舱25美元

取消费用:经济舱75美元,豪

基础大模型

退订要多少费用

LANGCHAIN-4

订要多少费用

向量数据:

退订要多少费用一

应用程序

EMBEDDING

向量数据库

资料,知识

用户信息:

三

数据

RAG

口十一?

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741158263487-85d26245-bcab-48ee-8d0c-45322c292687.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





 概念 

 向量： 

向量通常用来做相似性搜索，比如语义的一维向量，可以表示词语或短语的语义相似性。例如，“你好”、“hello”和“见到你很高兴”可以通过一维向量来表示它们的语义接近程度。

见到你很高兴[14

HELLO[13

(一维)

你好[12]

徐底

我叫

EEIIIHIEEEAN

EAEAAA

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741703979687-3dfe46b9-8bbc-4c8a-8ff7-11a63cd32800.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_33%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



然而，对于更复杂的对象，比如小狗，无法仅通过一个维度来进行相似性搜索。这时，我们需要提取多个特征，如颜色、大小、品种等，将每个特征表示为向量的一个维度，从而形成一个多维向量。例如，一只棕色的小型泰迪犬可以表示为一个多维向量 [棕色, 小型, 泰迪犬]。

![img](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741703733774-aafd18f8-6d1c-4f9c-ac0a-55eb5c2ef03e.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_44%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1500%2Climit_0)



如果需要检索见过更加精准， 我们肯定还需要更多维度的向量， 组成更多维度的空间，在多维向量空间中，相似性检索变得更加复杂。我们需要使用一些算法，如余弦相似度或欧几里得距离，来计算向量之间的相似性。向量数据库会帮我实现。

 文本向量化 

LangChain4j中来调用向量模型来对一句话进行向量化体验:



代码执行结果为：



从结果可以知道"你好，我叫徐庶"这句话经过OpenAiEmbeddingModel向量化之后得到的一个长度为1536的float数组。注意，1536是固定的，不会随着句子长度而变化。



那么，我们通过这种向量模型得到一句话对应的向量有什么作用呢？非常有用，因为我们可以基于向量来判断两句话之间的相似度，举个例子：

查询跟秋田犬类似的狗，  在向量数据库中根据每个狗的特点进行多维向量， 你会发现秋田犬的向量数值和柴犬的向量数值最接近， 就可以查到类似的狗。    （当然我这里只是举例，让你对向量数据库有一个印象）

(12,321,813,312)

(50,321,3213,654)

(中小黄色,温顺日本

(111,321,3213,987

(111,447,250,546

大小,颜色,性格,品种.)

(大小,颜色,性格,品科

(50,321,3213,654)

(中大,黑白,傻,俄罗斯

(大小颜色,性格,品种)

(中大,黄色,温顺,英国

(大小,颜色,性格,品种.)

(中小黄色温顺日本)

小颜色,性格,品种.

(小型,黄色,活泼,法

量数据库

秋田犬

哈士奇

泰迪

金毛

柴犬

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741159536310-23b0c084-513b-4801-9436-f63790fe47dc.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_30%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)







 向量数据库 

对于向量模型生成出来的向量，我们可以持久化到向量数据库，并且能利用向量数据库来计算两个向量之间的相似度，或者根据一个向量查找跟这个向量最相似的向量。

在LangChain4j中，EmbeddingStore表示向量数据库，它有支持[20+ 嵌入模型](https://docs.langchain4j.dev/integrations/embedding-stores/)：

| Embedding Store                                              | Storing Metadata | Filtering by Metadata | Removing Embeddings |
| ------------------------------------------------------------ | ---------------- | --------------------- | ------------------- |
| [In-memory](https://docs.langchain4j.dev/integrations/embedding-stores/in-memory) | ✅                | ✅                     | ✅                   |
| [Astra DB](https://docs.langchain4j.dev/integrations/embedding-stores/astra-db) | ✅                |                       |                     |
| [Azure AI Search](https://docs.langchain4j.dev/integrations/embedding-stores/azure-ai-search) | ✅                | ✅                     | ✅                   |
| [Azure CosmosDB Mongo vCore](https://docs.langchain4j.dev/integrations/embedding-stores/azure-cosmos-mongo-vcore) | ✅                |                       |                     |
| [Azure CosmosDB NoSQL](https://docs.langchain4j.dev/integrations/embedding-stores/azure-cosmos-nosql) | ✅                |                       |                     |
| [Cassandra](https://docs.langchain4j.dev/integrations/embedding-stores/cassandra) | ✅                |                       |                     |
| [Chroma](https://docs.langchain4j.dev/integrations/embedding-stores/chroma) | ✅                | ✅                     | ✅                   |
| [ClickHouse](https://docs.langchain4j.dev/integrations/embedding-stores/clickhouse) | ✅                | ✅                     | ✅                   |
| [Coherence](https://docs.langchain4j.dev/integrations/embedding-stores/coherence) | ✅                | ✅                     | ✅                   |
| [Couchbase](https://docs.langchain4j.dev/integrations/embedding-stores/couchbase) | ✅                |                       | ✅                   |
| [DuckDB](https://docs.langchain4j.dev/integrations/embedding-stores/duckdb) | ✅                | ✅                     | ✅                   |
| [Elasticsearch](https://docs.langchain4j.dev/integrations/embedding-stores/elasticsearch) | ✅                | ✅                     | ✅                   |
| [Infinispan](https://docs.langchain4j.dev/integrations/embedding-stores/infinispan) | ✅                |                       |                     |
| [Milvus](https://docs.langchain4j.dev/integrations/embedding-stores/milvus) | ✅                | ✅                     | ✅                   |
| [MongoDB Atlas](https://docs.langchain4j.dev/integrations/embedding-stores/mongodb-atlas) | ✅                | ✅                     | ✅                   |
| [Neo4j](https://docs.langchain4j.dev/integrations/embedding-stores/neo4j) | ✅                |                       |                     |
| [OpenSearch](https://docs.langchain4j.dev/integrations/embedding-stores/opensearch) | ✅                |                       |                     |
| [Oracle](https://docs.langchain4j.dev/integrations/embedding-stores/oracle) | ✅                | ✅                     | ✅                   |
| [PGVector](https://docs.langchain4j.dev/integrations/embedding-stores/pgvector) | ✅                | ✅                     | ✅                   |
| [Pinecone](https://docs.langchain4j.dev/integrations/embedding-stores/pinecone) | ✅                | ✅                     | ✅                   |
| [Qdrant](https://docs.langchain4j.dev/integrations/embedding-stores/qdrant) | ✅                | ✅                     | ✅                   |
| [Redis](https://docs.langchain4j.dev/integrations/embedding-stores/redis) | ✅                |                       |                     |
| [Tablestore](https://docs.langchain4j.dev/integrations/embedding-stores/tablestore) | ✅                | ✅                     | ✅                   |
| [Vearch](https://docs.langchain4j.dev/integrations/embedding-stores/vearch) | ✅                |                       |                     |
| [Vespa](https://docs.langchain4j.dev/integrations/embedding-stores/vespa) |                  |                       |                     |
| [Weaviate](https://docs.langchain4j.dev/integrations/embedding-stores/weaviate) | ✅                |                       | ✅                   |

其中有我们熟悉的几个数据库都可以用来存储向量，比如Elasticsearch、MongoDb、Neo4j、Pg、Redis。

下面通过[In-memory](https://docs.langchain4j.dev/integrations/embedding-stores/in-memory)方式演示完整使用流程：

 匹配向量 

在这个示例中， 我分别存储了预订航班和取消预订2段说明到向量数据库中

然后通过"退票要多少钱" 进行查询

些那用全在您的取间请求被处理后,在7个工作日内从应退还款项中扣除.如果您取消的射时间不在现定的则时间范围内(最晚在航班起飞前48小时)

俘到思款或者可能会有额外的罚金.请确保您按照航空公司的规定时间内提出取消请求,

数据嵌入阶段

VECTOR:0.23498.0.23084

据您提供的FUNNAIR服务条款,取消预订的费用如下:

TEXT0:WHEN.C.C

TEXT1WHEN.........

TEXT1.WITH...........

VECTOR:0.23498,0.23084

向量模型

EMBADDING

TEXT2CONVERT.............

数据检索阶段

UESTION:退票多少钱

QUESTION:退票多少钱

TEXTO:WHEN........

向量数据库

大语言模型

EMBEDDING

豪华经济能:50美元

-经济舱:75美元

商务0:25美元

INSERT

LLM

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741702267256-ccae55e5-3820-4f1a-8f52-feffc37affe7.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_40%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1393%2Climit_0)





代码执行结果为：



 由于我设置的是返回结果数量为1， 所以他会返回匹配度分数最高的那段内容.   如果返回结果数量为2. 其实预订航班那段也会查出来， 但是他的匹配度分数更低也没有太大的意义。



 知识库RAG演练 

VECTOR:[0.23498.0.23084.

知识库生成

VECTOR:0.23498.0.23084..

向量模型

根报系规供的凡UR服务条款.取消系订的费用如

TEXT2:CONVERT.OOOOOCOOOOO....

大语言模型

向量数据库

UESTION:退票多少钱

饮些赛用会春使的省请求被处理后,在7全

QUESTION:退票多少钱

RMBEDLDINGSHORTANDLONNO

EMBEDDING

TEXT1:WHEN.

TEXTO:WHEN

商务验:25美元

TEXT0:WHENO..

-爱华老济防50美元

-经济胎:75美元

TEXT1:WITH

INSERT

SPLIT

LLM

8身饮或者可能金有复外的始金.请码2区三

OOOOOOO

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741508132593-75cdae15-ba7f-4ab2-ab34-a4ed97ac3b72.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_29%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



 1[Document Loaders](https://docs.langchain4j.dev/category/document-loaders) 文档读取器VECTOR:[0.23498,0.23084.]TEXT1:WHENCO.....TEXT0:WHEN.......知识库生成TEXTLWTHOOOOOOOO..VECTOR:0.23498.0.23084TEXTOWHENTEXT2:CONVERT......EMBEDDINGBEDDINGSHORTANDLONGCONTEN向量数据库读取INSERTTSCOA..OOOO.OOSPLIT![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741186496899-c89b21c0-6e7b-4503-853f-9c0e109d3b1b.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_38%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp) 

 读取为文档



 1.1Document Parser  文档解析器 

如果要开发一个知识库系统，  这些资料可能在各种文件中， 比如word、txt、pdf、image、html等等， 所以langchain4j也提供了不同的文档解析器：



●TextDocumentParser来自 langchain4j 模块的 TextDocumentParser，它可以解析纯文本格式（e.g. TXT、HTML、MD 等）的文件。

●ApachePdfBoxDocumentParser来自langchain4j-document-parser-apache-pdfbox ，它可以解析 PDF 文件

●ApachePoiDocumentParser来自langchain4j-document-parser-apache-poi ，可以解析 MS Office 文件格式（e.g. DOC、DOCX、PPT、PPTX、XLS、XLSX 等）

●ApacheTikaDocumentParser来自 langchain4j-document-parser-apache-tika 模块中，可以自动检测和解析几乎所有现有的文件格式



在这里我来解析一份这个txt文件



terms-of-service.txt(1 kB)

， 所以我们用

TextDocumentParser



随便放这里吧

TERMS-OF-SERVICE.TXT

RESOURCES

RAG

L

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741186161707-403ad39e-0599-4511-b22a-36b5b7d3a8a2.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_10%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





代码：



 2[DocumentSplitter‌](https://docs.langchain4j.dev/tutorials/rag#document-splitter) 文档拆分器 

VECTOR:[0.23498,0.23084]

TEXT1:WHENOOOOOO......

TEXT0:WHE们....C*

VECTOR:[0.23498,0.23084]

知识库生成

TEXT1WITHOO

TEXT2:CONVERT.....-.....

TEXTOWHEN...

EMBEDDINGSHORTANDLONGCO

向量数据库

MBEDDING

INSERT

分割

SPLIT

OOAOOOOOOOOON.O

RTOOOOOOOON..

LPDFFILE

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741186628965-bab8cbb0-71d6-4cd5-bfab-0287b13a2540.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_38%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



由于文本读取过来后， 还需要分成一段一段的片段(分块chunk)， 分块是为了更好地拆分语义单元，这样在后面可以更精确地进行语义相似性检索，也可以避免LLM的Token限制。

langchain4j也提供了不同的文档拆分器：

| 分词器类型                  | 匹配能力                                | 适用场景                                                     |
| --------------------------- | --------------------------------------- | ------------------------------------------------------------ |
| ‌DocumentByCharacterSplitter‌ | 无符号分割                              | 就是严格根据字数分隔（不推荐，会出现断句）                   |
| ‌DocumentByRegexSplitter‌     | 正则表达式分隔                          | 根据自定义正则‌分隔                                           |
| ‌DocumentByParagraphSplitter‌ | 删除大段空白内容                        | 处理连续换行符（如段落分隔）（\\s*(?>\\R)\\s*(?>\\R)\\s*     |
| ‌DocumentByLineSplitter‌      | 删除单个换行符周围的空白， 替换一个换行 | （\\s*\\R\\s*） ●‌示例‌： ○输入文本："This is line one.\n\tThis is line two." ○使用 \s*\R\s* 替换为单个换行符："This is line one.\nThis is line two." |
| ‌DocumentByWordSplitter‌      | 删除连续的空白字符。                    | \\s+ ●‌示例‌： ○输入文本："Hello   World" ○使用 \s+ 替换为单个空格："Hello World" |
| ‌DocumentBySentenceSplitter‌  | 按句子分割                              | Apache OpenNLP 库中的一个类，用于检测文本中的句子边界。它能够识别标点符号（如句号、问号、感叹号等）是否标记着句子的末尾，从而将一个较长的文本字符串分割成多个句子。 |



这里我们选DocumentByLineSplitter‌吧， 因为内容不多， 所以其实没有特别大的关系， 后面如果大家有兴趣我详细讲解每一种的应用场景。



代码：

将第1步读取到的文档进行分割





chunk_size（块大小）指的就是我们分割的字符块的大小；chunk_overlap（块间重叠大小）就是下图中加深的部分，上一个字符块和下一个字符块重叠的部分，即上一个字符块的末尾是下一个字符块的开始。

3-CHUNK_OVERLAP

CHUNK_SIZE

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741770315404-78dd6500-6fbb-4d4d-97fd-e6fd341b6ea0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_51%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)





在使用按字符切分时，需要指定[分割符](https://so.csdn.net/so/search?q=分割符&spm=1001.2101.3001.7020)，另外需要指定块的大小以及块之间重叠的大小（允许重叠是为了尽可能地避免按照字符进行分割造成的语义损失）。

比如





整个流程如下：

![img](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741781839388-bdc761bf-84f6-4951-901d-5f620f80bee3.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_31%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1080%2Climit_0)



首先按照指定的分割符进行切分，切分过之后，如果块的长度小于 chunk_size 的大小，则进行块之间的合并。在进行合并时，遵循下面的规则：



1. 如果相邻块加在一起的长度小于或等于chunk_size，则进行合并；否则看你有没有子分割器，如果没有报错。





2. 在进行合并时，如果块的大小小于或等于chunk_overlap，并且和前后两个相邻块合并后，两个合并后的块均不超过chunk_size，则两个合并后的块允许有重叠



 在RAG系统中，文本分块的粒度需要平衡语义完整性与计算效率，并非越细越好。以下是关键考量点：



 2.1分隔经验： 

 2.1.1过细分块的潜在问题 

1‌语义割裂‌： 破坏上下文连贯性，影响模型理解‌ 。

2‌计算成本增加‌：分块过细会导致向量嵌入和检索次数增多，增加时间和算力开销‌。

3‌信息冗余与干扰‌：碎片化的文本块可能引入无关内容，干扰检索结果的质量，降低生成答案的准确性‌。

 2.1.2分块过大的弊端 

1‌信息丢失风险‌：过大的文本块可能超出嵌入模型的输入限制，导致关键信息未被有效编码‌。

2‌检索精度下降‌：大块内容可能包含多主题混合，与用户查询的相关性降低，影响模型反馈效果‌。

| ‌场景‌        | ‌分块策略‌                           | ‌参数参考‌         |
| ----------- | ---------------------------------- | ---------------- |
| 微博/短文本 | 句子级分块，保留完整语义           | 每块100-200字符‌  |
| 学术论文    | 段落级分块，叠加10%重叠            | 每块300-500字符‌  |
| 法律合同    | 条款级分块，严格按条款分隔         | 每块200-400字符‌  |
| 长篇小说    | 章节级分块，过长段落递归拆分为段落 | 每块500-1000字符‌ |



1‌固定长度分块‌

○‌字符数范围‌：通常建议每块控制在 ‌100-500字符‌（约20-100词），以平衡上下文完整性与检索效率‌12。

○‌重叠比例‌：相邻块间保留 ‌10-20%的重叠内容‌（如块长500字符时重叠50-100字符），减少语义断层‌34。

2‌语义分块‌

○‌段落或章节‌：优先按自然段落、章节标题划分，保持逻辑单元完整‌14。

○‌动态调整‌：对于长段落，可递归分割为更小单元（如先按段落分块，过长时再按句子拆分）‌46。

3‌专业领域调整‌

○‌高信息密度文本‌（如科研论文、法律文件）：采用更细粒度分块（100-200字符），保留专业术语细节‌14。

○‌通用文本‌（如新闻、社交媒体）：适当放宽分块大小（300-500字符）‌12

 



 3文本向量化 

知识库生成

VECTOR:0.23498.0.23084-

TEXT1:WHE门OOOOOOOOOOOOOOOO

VECTOR:0.23498.0.23084

向量模型

向量化

TEXT2:CONVERT

TEXTO:WHEN......

TEXTOWHEN

TEXT1:WITH.

向量数据库

EMBEDDING

INSERT

MBEDDINGSHORTAND

SPLIT

O...

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741188324988-536ef32b-5a6d-4999-9e08-df15d488a342.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_39%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



向量化存储之前在“文本向量化”介绍了， 就是通过向量模型库进行向量化



代码：

依然通过Qwen向量模型进行向量化：  将第2步分割的chunk进行向量化





 4存储向量 

VECTOR:0.23498.0.23084

EMBEDDING

VECTOR:[0.23498.0.23084

存储向量

TEXT0:WHENCOCOOOOO.......

知识库生成

TEXT1:WHEN*

MBEDDINGSHORTANDLONGCONTER

TEXT2:CONVERT.

TEXTU:WHER

TEXT1:WITR

INSERT

SPLIT

向量数据库

PDFFILE

OO

5555

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741187571248-9beae51f-e1ed-498a-8d8e-9c12d12936e1.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_38%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



选择向量数据库进行存储即可



代码：







 5向量数据库检索 



代码：

需要先将文本进行向量化， 然后去向量数据库查询，  







完整代码：







 6对话阶段 

些费用会在您的取消请求被处理后,在7个工作日内从应退还款项中扣除.如果您取消的时何不在规定的时间范围内(最晚在航斑起飞前4.小时)

VECTOR:[0.23498.0.23084.

VECTOR:[0.23498,0.23084..

得到限教成者可能全有板外的罚金,清降保您按照抗空公司的规定时间内提出取消请求.

根据您提供的FUNNAIR服务条款,取消预订的费用如下

向量模型

语言模型

知识库生成

EMBEDDINGSHORTANDLONGCONTENT

QUESTION:退票多小少钱

UESTION:退票多少钱

TEXTOWHEN.

MBEDDING

TEXT1:WHEH

TEXT2CONVERT

INSERT

向量数据库

TEXT1WITH...

TEXTOWHEN.

-经济舱:75美元

豪华经济舱50美元

SPLIT

OTTETTTATEAAT

团表示烟同营区监务款助际

商务舱:25美元

MEANG,THECOMPANEONWOULDNAURLYBEDONEONTHARLEVELWHERN

LLM

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741847699874-61b078b9-8d57-4f61-ac3d-5927d7bad5f6.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_117%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1500%2Climit_0)









AiService向量检索原理：

动态代理IMPLE接口

AISERVICES.BUILDER(接口.CLASS)

取消费用:经济舱75美元,豪华经济舱

基础大模型

退订要多少费用

向量模型

50美元,商务舱25美元.

应用程序

资料,知识

LANGCHAIN-4

退订要多少费用

退订要多少费用

向量数据:

ECTOR:12,32,4.]

向量数据库

MBEDDING

用户信息:

三

数据

RAG

![langchain4j (3).png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741849264761-669ff8b3-aaec-47ad-a84f-acaf970a1bc0.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_52%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp%2Fresize%2Cw_1500%2Climit_0)



 7整合SpringBoot 

最终其实还会将查询到的内容，  和对话上下文组合起来，  发给LLM为我们组织语言进行回答。

这一步我们直接整合进SpringBoot进行实战：

1 配置一个Content Retriever  内容检索器

a提供向量数据库和向量模型及其他参数

2将内容检索器绑定到AiServices 

3当我们进行LLM对话时， 底层会自动为我们检索向量数据库进行回答。



当然我们还需要提前存储向量数据到向量数据库



我们依然利用之前的/memory_stream_chat进行测试： 不需要改任何代码



完成：✿✿ヽ(°▽°)ノ✿

的时间不在规定的时间范围内(最晚在航班起飞前48小时),可能将不会得到退款或者

这些费用会在您的取消请求被处理后,在7个工作日内从应退还款项中扣除.如果您取消

能会有额外的罚金.请确保您按照航空公司的规定时间内提出取消请求.

据您提供的FUNNAIR服务条款,取消预订的费用如

COHTTP:/LOCALHOST808O/AIOTHER/MEMORYSTREAMCHATIMESSAGEE

豪华经济舱:50美元

商务舱:25美元

经济舱:75美元

517.14PXX539.43PX

T?MESSAGE=退票费用

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741189428931-0586255e-201b-42df-b897-398a47987140.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_25%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



 

希望大家能掌握利用langchan4j进行function-call和RAG开发。







 Chain多个ServiceAI   

在一个应用中， 可能需要多个模型共同一起协作完成一个任务。

为什么要这样：

您的LLM可能不需要始终了解您拥有的每个tools。例如，当用户只是向LLM打招呼或说再见时，让 LLM 访问数十或数百个tools的成本很高，有时甚至很危险（LLM 调用中包含的每个tools都会消耗大量token），并且可能会导致意想不到的结果（LLM 可能会产生幻觉或被操纵以使用非预期的输入来调用tools）。

关于 RAG：同样，有时需要为 LLM 提供一些上下文，但并非总是如此，因为它会产生额外的成本（更多上下文 = 更多token）并增加响应时间（更多上下文 = 更高的延迟）。

关于模型参数：在某些情况下，您可能想不通的对话使用不同的 LLM ，以利用不同LLM的最佳特性。



●您可以一个接一个地调用 AI 服务（又称链接-chain）。

●您可以使用确定性和 LLM 支持的if/else语句（AI 服务可以返回boolean）。

●您可以使用确定性和 LLM 支持的switch语句（AI 服务可以返回enum）。

●您可以使用确定性和 LLM 驱动的for/while循环（AI 服务可以返回int和其他数字类型）。

●您可以在单元测试中模拟 AI 服务（因为它是一个接口）。

●您可以单独地对每个 AI 服务进行集成测试。





并且我们可以自由的进行任务编排：

大家平常应该见过一些AI智能体， 由多个（LLM）任务组合编排为一个智能体， 

如果称.U川

正判断是香展示默认示例

Q支量聚合

G开给画8*

3默认示8

检出二返图克量

口结来

总入未记置输

![image.png](https://cdn.nlark.com/yuque/0/2025/png/22309163/1741916988603-46f63550-c86b-451b-a226-4456cc0a8240.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_44%2Ctext_5b6Q5bq26ICB5biI%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10%2Fformat%2Cwebp)



其实利用langchain4j的chain特性， 也可以完成这种类似的效果， 不过你需要自己完成前端编排以及不同任务的初始化和具体实现。 这不是一两句话能讲清楚的。 

这里我们利用langchain4j的chain特性你让你得到更多的灵感。 其实langchain4j也可以实现类似智能体，只不过目前没有特别优秀的开源项目，期待你去实现它！：