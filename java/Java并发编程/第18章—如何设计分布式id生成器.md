在分布式服务中，各种复杂的业务场景需要有一个用于做唯一标识的 id，例如订单业务、支付流水、聊天通信等业务场景。尤其是在分库分表场景中，分布式 id 生成器的使用频率更高。因此分布式 id 组件的设计应该要能支持以下几个特性：

-   **全局唯一特性**

这个点比较好理解，这里就不做过多的解释。

-   **组件递增特性**

可以是每个 id 都具有递增的特性，也可以是支持区间段内具备递增的特性。

-   **安全性**

有些重要的 id ，如果无意中暴露在了外网环境中，且没有做过安全防范，这其实是一件非常危险的事情。例如说，订单的 id 如果是由日期加订单数目的格式生成的话，如 2020111100001 表示 2020 年 11 月 11 日的第一笔订单，那么这样的数据就很容易被竞品对手获取到该公司的数据信息，从而产生数据安全漏洞的问题。

-   **高 qps**

在分布式集群中，id 生成器的调用其实是属于高并发请求，因此在设计的时候，对于性能的要求会比较高。

-   **高可用**

由于分布式 id 生成器是一个需要支持多个服务调用方共同使用的公共服务，一旦出现崩溃后果不堪设想，可能会导致大面积的业务线崩塌，所以在高可用方面，尤其需要慎重考虑。

## 业界常见的分布式 id 生成方案比对

### uuid

Java 程序中实现 uuid 的代码：

```java
String result = UUID.randomUUID().toString();
System.out.println(result);
```

生成的格式如下所示：

```
b0b2197d-bc8c-4fab-ad73-2b54e11b0869
```

uuid 的格式其实是会被 - 符号划分为五个模块，主要是 8-4-4-4-12 个字符拼接而成的一段字符串。采用这类 id 生成的数据，其实会存在一些安全隐患和性能问题。

当使用 uuid 生成的字符串作为 MySQL 索引时，在有大量数据并发写入时，会导致 b+ 树的叶子结点裂变频率加大，在裂变重组b+树叶子结点时，需要进行对节点数据的逐个字符的比对，其性能损耗也较高，应该抛弃该方案。

另外在早期版本的 uuid 中，是基于机器的 MAC 地址生成的。基于 MAC 地址生成 UUID 的算法可能会造成 MAC 地址泄露，著名的梅丽莎病毒就是利用了这个漏洞而出名的。

### **雪花算法**

SnowFlake 是 Twitter 公司采用的一种算法，目的是在分布式系统中产生全局唯一且趋势递增的 ID。

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/73e49d3cbb814e4c8b4301d262512fd3~tplv-k3u1fbpfcp-watermark.image?)


这里我稍微解释一下雪花算法的含义：

第一位通常是 0，没有特殊使用含义，因为 1 通常表示为补码。中间的 41 位是用于存储时间，41 位的长度足够容纳 69 年左右的时长。接下来的 10 位用于标示机器自身的 id，从而表示不同机器自身 id 的不同。

最后 12 位用于表示某一毫秒内的序列号，12位（bit）可以表示的最大正整数是 4096-1=4095，所以也就是说，一毫秒内可以同时生成 4095 个 id。

雪花算法生成的 id 中，时间戳位置和序列号位置还不能随意调整，因为要保证逐渐递增的特性。使用雪花算法生成 id 有好处也有不足之处，其好处有：**能够保证递增的特性，id 具有明确的含义，易懂**。不足之处为：**对于机器自身的系统时间有所依赖，一旦机器的系统时间发生了变化，在高并发环境下就有可能会有重复 id 生成的风险。**



前边我们所介绍的这些 id 生成方式都是与业务无关联性的，但有些场景是需要在 id 中注入些许业务信息的，例如在生成 id 时，加入一些包含业务含义的前缀。之前在工作中就有遇到过类似的设计，例如短信 id 和奖券 id：

```
sms_108678123
coupon_12908123
```
不过这种 id 前缀的设计应该要结合具体的业务场景来定。


### MongoDB 的主键 id 设计思路

其实在 MongoDB 里，也有往主键 id 中注入一些“基因”要素点的这类思路：


![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0dc7b4eb8eaf4e1da2042d1a1d3fd310~tplv-k3u1fbpfcp-watermark.image?)

在 MondoDB 中，它的主键 id 叫做 _id，其底层的存储结果是采用了 objectid 对象，在这个 objectid 里，包含了时间戳、宿主机的 ip、进程号码、自增号数值。
![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5bb4b054535b48a580947b3bd6300248~tplv-k3u1fbpfcp-watermark.image?)
在对这几种方案进行了调研之后，我们可以开始来着手实现一套分布式 id 生成器组件。

## **自研主要设计思路**

MySQL 配置分布式id的生成规则，负责 id 生成服务应用在启动的时候，将 id 生成规则从数据库拉取到本地，然后在本地缓存中提前生成一段 id 集合，后续请求直接从本地服务中提取即可。支持集群配置 id 生成器，能够支持高 qps 访问和较好的扩容性。


![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fa159303090f4ccebf2ad089a5a7bcd9~tplv-k3u1fbpfcp-watermark.image?)

配置表如下图所示：


![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/26e53b431e9d43e3b71eba7d9867dd44~tplv-k3u1fbpfcp-watermark.image?)

建表 sql：

```
CREATE TABLE `t_id_builder_config` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `desc` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT '描述',
  `init_num` bigint(13) DEFAULT NULL COMMENT 'id起步值',
  `next_threshold` bigint(20) DEFAULT NULL COMMENT '当前id所在阶段的阈值',
  `current_start` bigint(20) DEFAULT NULL COMMENT '当前id所在阶段的开始值',
  `step` int(11) DEFAULT NULL COMMENT 'id递增区间',
  `id_prefix` varchar(60) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT '业务前缀码，如果没有则返回时不携带',
  `version` int(11) NOT NULL DEFAULT '0' COMMENT '乐观锁版本号',
  `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
  `update_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

这里我对这张表的各个重要字段做些简单的介绍：

-   **init_num 字段** 是指id要从多少开始计算，这个和具体的业务场景有关，例如用户 id 的起步计算值就是一个不固定的数字。
-   **next_threshold 字段** 这个字段表示了当前各台 id 生成服务中，本地记录的最大 id 段的下一次更新值。
-   **current_start 字段** 这个字段表示了当前各台 id 生成服务中，本地记录的最大 id 段的开始值。
-   **step 字段** 代表了每个 id 生成器在本地缓存 id 的数量是多少。
-   **id_prefix 字段** 是一个业务前缀的表示功能，假设我们生成的 id 是希望有特殊的业务前缀的话，通过这个 id_prefix 来进行配置。


好了，现在我们已经将表给大概设计好了，但为了能够将这个组件给设计好，我们需要考虑一些极端情况下才会发生的问题，例如：

-   **当同时有多个请求访问 MySQL 获取 id 配置的时候，该如何防止并发问题？**

首先，考虑到各个节点同时进行并发访问 MySQL 进行本地 id 段更新的概率会很低，所以在对行信息进行更新时加入了 version 版本号字段，用于防止更新覆盖的情况。即使说更新失败，也会有 cas 的方式进行重试，重试超过一定次数之后直接中断。

-   **为何不引入 Redis 作为分布式锁，来防止并发修改数据库操作？**

不希望将该组件变得过于繁杂，减少系统对于第三方的依赖性 。

-   **假设本地 id 还没使用完，结果当前服务器宕机了，该如何预防？**

每次服务启动都需要更新表的配置，拉去最新的一批 id 集合到本地，这样就不会出现和之前 id 冲突的问题了。

-   **如何保证请求的高效性** **？**

数据提前在本地缓存中生成，取数的时候直接从本地缓存中提取即可。

-   **每次拉取到本地 id 段的大小该如何设计？**

这里我们先将本地 id 段简称为 segment，对于 segment 大小的设计，需要结合具体业务场景的并发量来进行预估。来看下面的这个案例。

我们的底层是计划设计一个定时任务去更新 segment 中的数据，这里我们假设更新的时间间隔为 X 秒。假设某时刻 segment 的数据达到了阈值，需要进行更新了，那么剩余的 segment 段至少是需要能够支撑超过 X 秒的，否则就会出现 id 生成失败的问题了。

## **代码实现步骤**

好了，前边我们有了一个基本的设计思路之后，再进行代码的实现就会比较简单了，下边让我们来看看如何通过实战代码实现分布式 id 生成器。

**核心代码思路**

首先是一个对外调用的 id 生成器接口设计部分，这块的实现我是分成了两种类型：

-   生成唯一 id (连续性递增)；
-   包含业务前缀的自增 id (连续性递增)。

```java
package 并发编程13.分布式id生成器.service;

/**
*  @Author idea
*  @Date created in 12:16 上午 2022/8/17
*/
public interface IdBuilderService {

    /**
* 根据本地步长度来生成唯一id(区间性递增)
*
*  @return
 */
Long increaseSeqId(int code);


    /**
* 根据本地步长度来生成唯一id(区间性递增)
*
*  @return
 */
String increaseSeqStrId(int code);

}
```

接下来，便是 id 生成器的具体实现模块，底层具体的细节部分，我在下边的代码中标明了注释。

```java
package 并发编程13.分布式id生成器.service.impl;

import lombok.extern.slf4j.Slf4j;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.InitializingBean;
import org.springframework.stereotype.Service;
import 并发编程13.分布式id生成器.bean.LocalSeqId;
import 并发编程13.分布式id生成器.bean.po.IdBuilderPO;
import 并发编程13.分布式id生成器.dao.mapper.IdBuilderMapper;
import 并发编程13.分布式id生成器.service.IdBuilderService;

import javax.annotation.Resource;
import java.util.ArrayList;
import java.util.BitSet;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;


/**
 * @Author idea
 * @Date created in 11:18 下午 2020/9/17
 */
@Service
@Slf4j
public class IdBuilderServiceImpl implements IdBuilderService, InitializingBean {

    private static ConcurrentHashMap<Integer, BitSet> bitSetMap = new ConcurrentHashMap<>();
    private static Map<Integer, LocalSeqId> localSeqMap;

    private final static Logger LOGGER = LoggerFactory.getLogger(IdBuilderServiceImpl.class);

    @Resource
    private IdBuilderMapper idBuilderMapper;

    /**
     * 考虑分布式环境下 多个请求同时更新同一行数据的情况
     *
     * @param idBuilderPO
     * @return
     */
    private IdBuilderPO updateIdBuilderConfig(IdBuilderPO idBuilderPO) {
        int updateResult = -1;
        //假设重试过程中出现网络异常，那么使用cas的时候必须要考虑退出情况 极限情况下更新100次
        for (int i = 0; i < 100; i++) {
            try {
                IdBuilderPO newIdBuilderPO = idBuilderMapper.selectOne(idBuilderPO.getId());
                long nextThreshold = newIdBuilderPO.getNextThreshold();
                long step = newIdBuilderPO.getStep();
                updateResult = idBuilderMapper.updateCurrentThreshold(nextThreshold + step, nextThreshold, newIdBuilderPO.getId(), newIdBuilderPO.getVersion());
                if (updateResult > 0) {
                    return newIdBuilderPO;
                }
            } catch (Exception e) {
                LOGGER.error("[updateIdBuilderConfig] error is ", e);
            }
        }
        return null;
    }


    @Override
    public void afterPropertiesSet() {
        //在启动环境，预先初始化好id数据
        List<IdBuilderPO> idBuilderPOS = idBuilderMapper.selectAll();
        List<IdBuilderPO> refreshList = new ArrayList<>();
        for (IdBuilderPO idBuilderPO : idBuilderPOS) {
            //每次重启到时候，都需要将之前的上一个区间的id全部抛弃，使用新的步长区间
            refreshList.add(updateIdBuilderConfig(idBuilderPO));
        }
        localSeqMap = new ConcurrentHashMap<>(refreshList.size());
        for (IdBuilderPO idBuilderPO : refreshList) {
            //有序的id段由一个LocalSeqId管理
            LocalSeqId localSeqId = new LocalSeqId();
            localSeqId.setNextUpdateId(new AtomicLong(idBuilderPO.getNextThreshold()+idBuilderPO.getStep()));
            localSeqId.setCurrentId(new AtomicLong(idBuilderPO.getNextThreshold()));
            localSeqId.setStep(idBuilderPO.getStep());
            localSeqId.setIdPrefix(idBuilderPO.getIdPrefix());
            localSeqMap.put(idBuilderPO.getId(), localSeqId);
        }
        //自动预先加载新的id段到本地缓存中
        new Thread(new refreshLocalIdCacheJob()).start();
    }


    /**
     * 内部判断是否需要刷新本地id步长数据
     */
    class refreshLocalIdCacheJob implements Runnable {

        @Override
        public void run() {
            while (true) {
                try {
                    Thread.sleep(1000);
                    for (Integer code : localSeqMap.keySet()) {
                        //这里做一个校验,确认此时本地id段确实需要更新
                        LocalSeqId localSeqId = localSeqMap.get(code);
                        if (localSeqId.getNextUpdateId().get() - localSeqId.getCurrentId().get() > localSeqId.getStep() * 0.25) {
                            continue;
                        }
                        int updateResult = -1;
                        //如果更新失败，进行重试
                        for (int i = 0; i < 100; i++) {
                            IdBuilderPO newIdBuilderPO = idBuilderMapper.selectOne(code);
                            long nextThreshold = newIdBuilderPO.getNextThreshold();
                            long currentStart = newIdBuilderPO.getCurrentStart();
                            long step = newIdBuilderPO.getStep();
                            updateResult = idBuilderMapper.updateCurrentThreshold(nextThreshold + step, currentStart + step, code, newIdBuilderPO.getVersion());
                            if (updateResult > 0) {
                                localSeqId = new LocalSeqId();
                                localSeqId.setCurrentId(new AtomicLong(nextThreshold + 1));
                                localSeqId.setIdPrefix(newIdBuilderPO.getIdPrefix());
                                localSeqId.setStep(newIdBuilderPO.getStep());
                                localSeqId.setNextUpdateId(new AtomicLong(nextThreshold + step));
                                localSeqMap.put(code, localSeqId);
                                LOGGER.info("更新id本地步长成功");
                                break;
                            }
                        }
                    }
                } catch (InterruptedException e) {
                    LOGGER.error("e is ", e);
                }
            }
        }
    }

    @Override
    public Long increaseSeqId(int code) {
        //直接从本地缓存中提取id数据
        LocalSeqId localSeqId = localSeqMap.get(code);
        if (localSeqId == null) {
            LOGGER.error("[increaseSeqId] code:{} is error", code);
            return null;
        }
        //原子性自增操作
        long result = localSeqId.getCurrentId().getAndAdd(1);
        return result;
    }

    @Override
    public String increaseSeqStrId(int code) {
        LocalSeqId localSeqId = localSeqMap.get(code);
        if (localSeqId == null) {
            LOGGER.error("[increaseSeqStrId] code:{} is error", code);
            return null;
        }
        //原子性自增操作
        long result = localSeqId.getCurrentId().getAndAdd(1);
        return localSeqId.getIdPrefix() + result;
    }
}
```

数据库层面设计：

```java
@Mapper
public interface IdBuilderMapper extends BaseMapper<IdBuilderPO> {

    @Select("select * from t_id_builder_config")
    List<IdBuilderPO> selectAll();

    @Select("select * from t_id_builder_config where id=#{id} limit 1 ")
    IdBuilderPO selectOne(@Param("id") int id);

    @Update("UPDATE t_id_builder_config set next_threshold=#{nextThreshold},current_start=#{currentStart},version=version+1 where id=#{id} and version=#{version}")
    Integer updateCurrentThreshold(@Param("nextThreshold") long nextThreshold, @Param("currentStart") long currentStart, @Param("id") int id, @Param("version") int version);

}
```

这里面我只贴出了部分核心代码，HTTP 和 RPC 访问部分其实大同小异，可以根据自己的需要进行额外定制。

下边我贴出关于 controller 部分的代码：

```
@RestController
@RequestMapping(value = "id-builder")
public class IdBuilderController {

    @Autowired
    private IdBuilderService idBuilderService;


    @GetMapping("increase-seq-id")
    public Long increaseSeqId(int code){
        long result = idBuilderService.increaseSeqId(code);
        System.out.println(result);
        return result;
    }

    @GetMapping("increase-seq-str-id")
    public String increaseSeqStrId(int code){
        String result = idBuilderService.increaseSeqStrId(code);
        System.out.println(result);
        return result;
    }

}

```

application.propertier 配置文件。

```
spring.application.name=JdbcApplication
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.url=jdbc:mysql://localhost:3306/idea_test?useUnicode=true&characterEncoding=utf-8
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

mybatis-plus.configuration.map-underscore-to-camel-case=true
mybatis.mapper-locations:classpath*:mapper/UserMapper.xml
logging.level.com.example.demo.mapper=debug

server.tomcat.max-threads=1000
server.tomcat.max-connections=4000
```

ps：由于我是采用了 HTTP 请求的方式进行测试，所以才会去增加了 tomcat 的参数配置。

## **压力测试环节**

通过将服务打包部署在机器上边，这里我选择的是对单个节点进行压力测试，采用的是ab命令去发送，使用的是1core+2gb机器。测试的结果如下所示：

```java
>>>> ab -n1000 -c1000 'api.test.cc:8080/id-builder/increase-seq-id?code=8'

This is ApacheBench, Version 2.3 <$Revision: 1879490 $>

Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/

Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking localhost (be patient)

Completed 100 requests

Completed 200 requests

Completed 300 requests

Completed 400 requests

Completed 500 requests

Completed 600 requests

Completed 700 requests

Completed 800 requests

Completed 900 requests

Completed 1000 requests

Finished 1000 requests

Server Software:

Server Hostname: localhost

Server Port: 8080

Document Path: /id-builder/increase-seq-id?code=8

Document Length: 5 bytes

Concurrency Level: 1000

Time taken for tests: 0.106 seconds

Complete requests: 1000

Failed requests: 0

Total transferred: 110000 bytes

HTML transferred: 5000 bytes

Requests per second: 9392.23 [#/sec] (mean)

Time per request: 106.471 [ms] (mean)

Time per request: 0.106 [ms] (mean, across all concurrent requests)

Transfer rate: 1008.93 [Kbytes/sec] received

Connection Times (ms)

min mean[+/-sd] median max

Connect: 0 31 5.0 32 40

Processing: 20 31 9.0 26 54

Waiting: 0 29 10.2 24 54

Total: 40 62 6.4 60 85

Percentage of the requests served within a certain time (ms)

50% 60

66% 63

75% 64

80% 66

90% 70

95% 75

98% 81

99% 83

100% 85 (longest request)
```
压测启动后，后台控制台会打印相关系列参数，这里我对单节点的压力测试是施加了 1000次/s 的压力。如果日后业务流量的压力增大的话，可以通过扩增服务节点的方式来增加承载能力。


另外，在设计的时候，由于不同的机器节点会维护不同的 id 段，所以当需要扩增机器的时候，新加的机器不会对原有 id 段造成影响，可以支持横向扩容。


## 课后小结

在本章节中，我们主要介绍了分布式 id 生成器的设计思路，以及如何通过代码去落地实现这样的一款组件。在设计这款组件的过程中，我们采用了从本地缓存来生成 id 的方式来保证接口的高性能。

同时在设计的时候，利用了表配置的方式支持多机器同时生成不同段的 id 数据，从而提升了该服务的吞吐能力。

最后在接口调用测试环境，采用的是  Nginx 加上 Tomcat 的方式进行压力测试，不过这部分的设计可以结合现有的业务系统进行改造。例如，如果业务场景中使用的是 Dubbo 服务，则不需要通过 Nginx 进行负载均衡，直接暴露 Dubbo 接口即可，而且在普遍场景中 Dubbo 的调用速率要明显高于 HTTP 调用的速率。




## **课后思考**

在上一章节中，我们介绍了基于 Dubbo、SpringBoot、线程池应用场景下，分布式链路 id 是如何落地的，然后留下了一道实战思考题，如何在使用 RocketMQ 的时候，也将链路 id 给串联起来。

这部分的实战代码其实已经上传到了 Gitee 仓库中。其核心的思想是利用了 RocketMQ 的钩子函数来实现的，在消息发送和接收的时候，完成链路 id 的透传，这部分的细节源代码地址如下：

```
 https://gitee.com/IdeaHome_admin/concurrence-programming-lession/tree/master/src/main/java/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B12
```

另外在我个人的公众号上，也有专门的一篇文章介绍了如何利用RocketMQ的钩子函数来实现全链路id的透传功能，里边涉及到了源代码的底层分析，感兴趣的读者可以点击[这里](https://mp.weixin.qq.com/s/7urxXmtk1VZ9CtFvIssXKQ)前往阅读。

**本节课思考**

大家在工作的什么场景中会使用到分布式id生成器呢，欢迎大家在评论区进行讨论。

